{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91443f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config as CFG\n",
    "from CLIP import CLIPModel\n",
    "\n",
    "from fmcib.preprocessing import get_transforms\n",
    "from monai.data import CSVDataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import monai\n",
    "from fmcib.models import fmcib_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212d6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CLIPDatasetImg(CSVDataset):\n",
    "    def __init__(self, coord_path):\n",
    "        self.transform = get_transforms()\n",
    "        coord_csv = pd.read_csv(coord_path)\n",
    "        self.pid = pd.read_csv('/workspace/radraid/projects/imaging_biomarker/CLIP_nodule/dataset_csv/semantic_label.csv')[['pid','nodule_id']]\n",
    "        super().__init__(coord_csv, transform=self.transform)\n",
    "    def __getitem__(self, idx):\n",
    "        img_processed = super().__getitem__(idx)\n",
    "        return img_processed\n",
    "    \n",
    "def build_loaders(coord_path):\n",
    "    dataset = CLIPDatasetImg(\n",
    "        coord_path\n",
    "    )\n",
    "    dataloader = monai.data.DataLoader(dataset, \n",
    "                          batch_size=1, \n",
    "                          num_workers=CFG.num_workers, \n",
    "                          shuffle=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "633353c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings( model_path, coord_path, save_path = None):\n",
    "    loader = build_loaders(coord_path)\n",
    "    pid = loader.dataset.pid.pid.astype(int).astype(str)\n",
    "    nodule_id = loader.dataset.pid.nodule_id.astype(int).astype(str)\n",
    "    if model_path == 'foundation':\n",
    "        model = fmcib_model().to(CFG.device)\n",
    "    else:\n",
    "        semantic_embedding = 64#valid_loader.dataset.semantic_features.shape[1]\n",
    "        model = CLIPModel(semantic_embedding = semantic_embedding).to(CFG.device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=CFG.device)['model'])\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(loader)):\n",
    "            if model_path == 'foundation':\n",
    "                image_embeddings = model(batch.to(CFG.device))\n",
    "            else:\n",
    "                image_features = model.image_encoder(batch.to(CFG.device))\n",
    "                image_embeddings = model.image_projection(image_features)\n",
    "            if save_path is not None:\n",
    "                torch.save(image_embeddings, os.path.join(save_path,f'{pid[i]}_{nodule_id[i]}.pt' ))\n",
    "\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "            \n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d909b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 09:35:29.666 | WARNING  | fmcib.models.load_model:load:104 - Missing keys: [] and unexpected keys: []\n",
      "2024-04-24 09:35:29.669 | INFO     | fmcib.models.load_model:load:129 - Loaded pretrained model weights \n",
      "\n",
      "1167it [47:07,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "m , emb = get_image_embeddings('/workspace/radraid/projects/imaging_biomarker/CLIP_nodule/results_loss/bz4_j10_lr1e-05_wd0.001_pd256_dropout0.1_epochs100_ga1_general_internal_external_freeze/best.pt',\n",
    "                               CFG.coord_path,\n",
    "                               '/workspace/radraid/projects/imaging_biomarker/CLIP_nodule/results_downstream/bz4_j10_lr1e-05_wd0.001_pd256_dropout0.1_epochs100_ga1_general_internal_external_freeze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad4993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec19aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDatasetImg(CSVDataset):\n",
    "    def __init__(self, coord_path):\n",
    "        self.transform = get_transforms()\n",
    "        coord_csv = pd.read_csv(coord_path)\n",
    "        self.pid = coord_csv.pid.values\n",
    "        super().__init__(coord_csv, transform=self.transform)\n",
    "    def __getitem__(self, idx):\n",
    "        img_processed = super().__getitem__(idx)\n",
    "        return img_processed\n",
    "def build_loaders(coord_path):\n",
    "    dataset = CLIPDatasetImg(\n",
    "        coord_path\n",
    "    )\n",
    "    dataloader = monai.data.DataLoader(dataset, \n",
    "                          batch_size=1, \n",
    "                          num_workers=CFG.num_workers, \n",
    "                          shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "def get_image_embeddings( model_path, coord_path, save_path = None):\n",
    "    loader = build_loaders(coord_path)\n",
    "    pid = loader.dataset.pid.astype(str)\n",
    "    if model_path == 'foundation':\n",
    "        model = fmcib_model().to(CFG.device)\n",
    "    else:\n",
    "        semantic_embedding = 64#valid_loader.dataset.semantic_features.shape[1]\n",
    "        model = CLIPModel(semantic_embedding = semantic_embedding).to(CFG.device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=CFG.device)['model'])\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(loader)):\n",
    "            if model_path == 'foundation':\n",
    "                image_embeddings = model(batch.to(CFG.device))\n",
    "            else:\n",
    "                image_features = model.image_encoder(batch.to(CFG.device))\n",
    "                image_embeddings = model.image_projection(image_features)\n",
    "            if save_path is not None:\n",
    "                torch.save(image_embeddings, os.path.join(save_path,f'{pid[i]}.pt' ))\n",
    "\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "            \n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ef0069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 16:46:11.411 | WARNING  | fmcib.models.load_model:load:104 - Missing keys: [] and unexpected keys: []\n",
      "2024-04-24 16:46:11.418 | INFO     | fmcib.models.load_model:load:129 - Loaded pretrained model weights \n",
      "\n",
      "0it [00:00, ?it/s]Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "56it [01:48,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m , emb = get_image_embeddings('/workspace/radraid/projects/imaging_biomarker/CLIP_nodule/results_loss/bz4_j10_lr1e-05_wd0.001_pd256_dropout0.1_epochs100_ga1_general_internal_external_freeze/best.pt',\n",
    "                               '/workspace/radraid/projects/imaging_biomarker/CLIP_nodule/dataset_csv/ucla_path_label.csv',\n",
    "                               '/workspace/radraid/projects/imaging_biomarker/CLIP_nodule/results_downstream/bz4_j10_lr1e-05_wd0.001_pd256_dropout0.1_epochs100_ga1_general_internal_external_freeze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6854694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1b122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
